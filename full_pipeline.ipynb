{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0864029",
   "metadata": {},
   "source": [
    "# Orchestrating Machine Learning Workflow with Amazon SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd7a95",
   "metadata": {},
   "source": [
    "## Using Bring your own code method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c998a5b8",
   "metadata": {},
   "source": [
    "###### What is a workflow in Machine Learning?\n",
    "A workflow in ML is a sequence of tasks that runs subsequently in the machine learning process.\n",
    "\n",
    "The workflows are the different phases of a machine learning project. These phases include:\n",
    "\n",
    "data collection, \n",
    "data pre-processing, \n",
    "building datasets, \n",
    "model training and refinement, \n",
    "evaluation, \n",
    "deployment to production.\n",
    "\n",
    "Machine Learning workflows can differ from company to company, so to get the most accurate one, letâ€™s look at few tech giants on how they define them for their own use.\n",
    "A machine learning workflow describes the processes involved in machine learning work.\n",
    "\n",
    "Machine Learning workflow is a combination of the defined steps in a specific succession. It starts with defining problems and processes through Data preparation, Algorithm Selection, Training Model, Testing, and Evaluation respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef335a6b",
   "metadata": {},
   "source": [
    "###### What is pipeline in Machine Learning?\n",
    "Pipelines in machine learning are an infrastructural medium for the entire ML workflow. Pipelines help automate the overall MLOps workflow, from data gathering, EDA, data augmentation, to model building and deployment. After the deployment, it also supports reproduction, tracking, and monitoring.\n",
    "\n",
    "ML pipelines help improve the performance and management of the entire model, resulting in quick and easy deployment.\n",
    "\n",
    "A machine learning pipeline is a way to codify and automate the workflow it takes to produce a machine learning model. Machine learning pipelines consist of multiple sequential steps that do everything from data extraction and preprocessing to model training and deployment.\n",
    "\n",
    "A machine learning pipeline is used to help automate machine learning workflows. They operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative.\n",
    "Machine learning (ML) pipelines consist of several steps to train a model. Machine learning pipelines are iterative as every step is repeated to continuously improve the accuracy of the model and achieve a successful algorithm. To build better machine learning models, and get the most value from them, accessible, scalable and durable storage solutions are imperative, paving the way for on-premises object storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5583ef7",
   "metadata": {},
   "source": [
    "###### What is Sagemaker Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed80ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A Sagemaker Pipeline\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c33884",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/blastchar/telco-customer-churn\n",
    "https://www.kaggle.com/bhartiprasad17/customer-churn-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47da64",
   "metadata": {},
   "source": [
    "Data ingestion part is the first step of this notebook and upload the downloaded dataset into S3 bucket. You can select our own data set for the **`input_path`** as is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81968d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "input_path = f\"s3://{default_bucket}/customer_churn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "\n",
    "df.to_csv(\"s3://sagemaker-ap-southeast-1-120582440665/customer_churn/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdc63634",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset_path = os.path.join(input_path, \"WA_Fn-UseC_-Telco-Customer-Churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Install predefined Sagemaker libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "579e035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput,TransformInput,CreateModelInput\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.conditions import (\n",
    "    ConditionGreaterThanOrEqualTo,\n",
    ")\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource,\n",
    "    ModelMetrics,\n",
    ")\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    "    TransformStep,\n",
    "    CreateModelStep\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.model import Model\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5550724c",
   "metadata": {},
   "source": [
    "Next step is start session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d7c92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session(region, default_bucket):\n",
    "    \"\"\"Gets the sagemaker session based on the region.\n",
    "    Args:\n",
    "        region: the aws region to start the session\n",
    "        default_bucket: the bucket to use for storing the artifacts\n",
    "    Returns:\n",
    "        `sagemaker.session.Session instance\n",
    "    \"\"\"\n",
    "\n",
    "    boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "    sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "    runtime_client = boto_session.client(\"sagemaker-runtime\")\n",
    "    return sagemaker.session.Session(\n",
    "        boto_session=boto_session,\n",
    "        sagemaker_client=sagemaker_client,\n",
    "        sagemaker_runtime_client=runtime_client,\n",
    "        default_bucket=default_bucket,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Define Parameters to Parametrize Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "162e30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for pipeline execution\n",
    "processing_instance_count = ParameterInteger(\n",
    "        name=\"ProcessingInstanceCount\", default_value=1\n",
    "    )\n",
    "processing_instance_type = ParameterString(\n",
    "        name=\"ProcessingInstanceType\", default_value=\"ml.t3.medium\"\n",
    "    )\n",
    "postprocessing_instance_type = ParameterString(\n",
    "        name=\"PostProcessingInstanceType\", default_value=\"ml.m5.2xlarge\"\n",
    "    )\n",
    "        \n",
    "preprocess_output_data = ParameterString(\n",
    "        name=\"InputDataUrl\",\n",
    "        default_value= os.path.join(input_path, \"preprocess_output_data/\"),\n",
    "    )\n",
    "    \n",
    "training_instance_type = ParameterString(\n",
    "        name=\"TrainingInstanceType\", default_value=\"ml.m5.12xlarge\"\n",
    "    )\n",
    "    \n",
    "model_path = ParameterString(\n",
    "        name=\"ModelPath\",\n",
    "        default_value=os.path.join(input_path, \"model_path/\"), \n",
    "    )\n",
    "\n",
    "inference_instance_type = ParameterString(\n",
    "        name=\"InferenceInstanceType\", default_value=\"ml.m5.large\"\n",
    "    )\n",
    "    \n",
    "inference_output_data=ParameterString(\n",
    "        name=\"InferencePath\",\n",
    "        default_value=os.path.join(input_path, \"inference_output_data/\"),\n",
    "    )\n",
    "    \n",
    "model_approval_status = ParameterString(\n",
    "        name=\"ModelApprovalStatus\",\n",
    "        default_value=\"Approved\",  # ModelApprovalStatus can be set to a default of \"Approved\" if you don't want manual approval.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fadb912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocessing/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing/preprocessing.py\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def change_format(df):\n",
    "    df['TotalCharges'] = pd.to_numeric(df.TotalCharges, errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def missing_value(df):\n",
    "    print(\"count of missing values: (before treatment)\", df.isnull().sum())\n",
    "    \n",
    "    df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].mean())\n",
    "    print(\"count of missing values: (before treatment)\", df.isnull().sum())\n",
    "    print(\"missing values successfully replaced\")\n",
    "    return df\n",
    "\n",
    "def data_manipulation(df):\n",
    "    df = df.drop(['customerID'], axis = 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cat_encoder(df, variable_list):\n",
    "    dummy = pd.get_dummies(df[variable_list], drop_first = True)\n",
    "    df = pd.concat([df, dummy], axis=1)\n",
    "    df.drop(df[cat_var], axis = 1, inplace = True)\n",
    "    \n",
    "    print(\"Encoded successfully\")\n",
    "    return df\n",
    "\n",
    "def scaling(X):  \n",
    "    min_max=MinMaxScaler()\n",
    "    X=pd.DataFrame(min_max.fit_transform(X),columns=X.columns)\n",
    "    \n",
    "    return X\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-test-split-ratio\", type=float, default=0.3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", 'WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "\n",
    "    print(\"Reading input data from {}\".format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    \n",
    "    columns = ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
    "           'tenure', 'PhoneService', 'MultipleLines', 'InternetService',\n",
    "           'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
    "           'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "           'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
    "\n",
    "    df.columns = columns\n",
    "\n",
    "    cat_var = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService',\n",
    "           'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
    "           'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "           'PaymentMethod', 'Churn']\n",
    "    \n",
    "    df = data_manipulation(missing_value(change_format(df)))\n",
    "    df = cat_encoder(df, cat_var)\n",
    "\n",
    "    X = df.iloc[:, 0:30]\n",
    "    y = df.iloc[:, -1]\n",
    "    X = scaling(X)\n",
    "    \n",
    "    print(\"Saving the outputs\")\n",
    "    X_output_path = os.path.join(\"/opt/ml/processing/train\", \"X.csv\")   \n",
    "        \n",
    "    print(\"Saving output to {}\".format(X_output_path))\n",
    "    pd.DataFrame(X).to_csv(X_output_path, header=False, index=False)\n",
    "    \n",
    "    y_output_path = os.path.join(\"/opt/ml/processing/test\", \"y.csv\")   \n",
    "        \n",
    "    print(\"Saving output to {}\".format(y_output_path))\n",
    "    pd.DataFrame(y).to_csv(y_output_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e46dd24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75405347",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repository = \"cutomer-churn-preprocessing-image\"\n",
    "tag = \":latest\"\n",
    "uri_suffix = \"amazonaws.com\"\n",
    "    \n",
    "preprocessing_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "        account_id, region, uri_suffix, ecr_repository + tag\n",
    "    )\n",
    "        \n",
    "script_processor = ScriptProcessor(\n",
    "         command = [\"python3\"],\n",
    "         image_uri = preprocessing_repository_uri,\n",
    "         role = role,\n",
    "         instance_count = 1,\n",
    "         instance_type = processing_instance_type,\n",
    "         #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Preprocessing'}],\n",
    "         #network_config = NetworkConfig(subnets=subnets.split(':'), security_group_ids=security_group_ids.split(':'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_tags=[{'Key': 'EnvironmentName', 'Value': env}, {'Key': 'ProjectName', 'Value': 'project_name'},{'Key': 'DepartmentName', 'Value': 'DepartmentName'}, {'Key': 'UsecaseName', 'Value': \"project_name\"}, {'Key': 'ResourceName', 'Value': 'sagemaker_notebook'}, {'Key': 'OwnerName', 'Value': 'mlops'}, {'Key': 'BUName', 'Value': 'mobile'} ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6633b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3://dlk-cloud-tier-10-preprocessed-ml-dev/PREPAID_CHURN/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "X_output_data = \"s3://dlk-cloud-tier-10-preprocessed-ml-dev/PREPAID_CHURN/X/\"\n",
    "y_output_path = \"s3://dlk-cloud-tier-10-preprocessed-ml-dev/PREPAID_CHURN/y/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = os.path.join(input_path,\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "X_output_data = os.path.join(input_path,\"X/\")\n",
    "y_output_path = os.path.join(input_path,\"y/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f747516",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_preprocess = ProcessingStep(\n",
    "    name=\"customer-churn-preprocessing\",\n",
    "    processor=script_processor,\n",
    "    code=\"preprocessing/preprocessing.py\",\n",
    "    inputs=[ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "            ProcessingOutput(output_name=\"train\", destination=X_output_data, source=\"/opt/ml/processing/train\"),\n",
    "            ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\", destination=y_output_path),\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2befe854",
   "metadata": {},
   "source": [
    "## Define a Training Step to Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2e68769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training/model/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/model/train.py\n",
    "#Import the neccessary libaries in here\n",
    "import os\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier,plot_importance\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.combine import SMOTETomek # doctest: +NORMALIZE_WHITESPACE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, auc,roc_curve,r2_score,confusion_matrix,roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import argparse\n",
    "import pickle\n",
    "import boto3\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-test-split-ratio\", type=float, default=0.3)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"Received arguments {}\".format(args))\n",
    "    training_data_directory = '/opt/ml/input/data/training/'\n",
    "    training_data_directory2 = '/opt/ml/input/data/test/'\n",
    "    train_features_data = os.path.join(training_data_directory, \"X.csv\")\n",
    "    train_labels_data = os.path.join(training_data_directory2, \"y.csv\")\n",
    "    print(\"Reading input data\")\n",
    "    print(\"Reading input data from {}\".format(train_features_data))\n",
    "    X = pd.read_csv(train_features_data, header = None)\n",
    "    \n",
    "    print(\"Reading input data from {}\".format(train_labels_data))\n",
    "    y = pd.read_csv(train_labels_data, header = None)\n",
    "    \n",
    "    columns = ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges',\n",
    "       'gender_Male', 'Partner_Yes', 'Dependents_Yes', 'PhoneService_Yes',\n",
    "       'MultipleLines_No phone service', 'MultipleLines_Yes',\n",
    "       'InternetService_Fiber optic', 'InternetService_No',\n",
    "       'OnlineSecurity_No internet service', 'OnlineSecurity_Yes',\n",
    "       'OnlineBackup_No internet service', 'OnlineBackup_Yes',\n",
    "       'DeviceProtection_No internet service', 'DeviceProtection_Yes',\n",
    "       'TechSupport_No internet service', 'TechSupport_Yes',\n",
    "       'StreamingTV_No internet service', 'StreamingTV_Yes',\n",
    "       'StreamingMovies_No internet service', 'StreamingMovies_Yes',\n",
    "       'Contract_One year', 'Contract_Two year', 'PaperlessBilling_Yes',\n",
    "       'PaymentMethod_Credit card (automatic)',\n",
    "       'PaymentMethod_Electronic check', 'PaymentMethod_Mailed check']\n",
    "    #df_train_labels = pd.read_csv(train_labels_path)\n",
    "\n",
    "    #print(\"Loading validation dataframes...\")\n",
    "    #df_val_features = pd.read_csv(val_features_path)\n",
    "    #df_val_labels = pd.read_csv(val_labels_path)\n",
    "    \n",
    "    X.columns = columns\n",
    "    \n",
    "    column = ['Churn']\n",
    "    \n",
    "    y.columns = column\n",
    "    \n",
    "    print(\"Successfully rename the dataset\")\n",
    "    \n",
    "    print(\"split the dataset\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=5)\n",
    "    \n",
    "    print(\"train the model\")\n",
    "    xgb = XGBClassifier()\n",
    "    parameters = {\n",
    "        'n_estimators': [100, 250, 500],\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate' : [0.01, 0.05, 0.1],\n",
    "        'gamma' : [0.0, 0.1, 0.2],\n",
    "        'min_child_weight' : [1, 3]\n",
    "    }\n",
    "    \n",
    "    cv = GridSearchCV(xgb, parameters, cv=3)\n",
    "    \n",
    "    print(\"fitting the model\")\n",
    "    cv.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    final_model = cv.best_estimator_\n",
    "\n",
    "    y_pred = final_model.predict(X_test)\n",
    "\n",
    "    print(confusion_matrix(y_test,final_model.predict(X_test)))\n",
    "\n",
    "    print(classification_report(y_test,y_pred))\n",
    "\n",
    "    print(roc_auc_score(y_test,final_model.predict_proba(X_test)[:, 1]))\n",
    "    \n",
    "    OUTPUT_DIR = \"/opt/ml/model/\"\n",
    "    \n",
    "    print(\"Saving model....\")\n",
    "            \n",
    "    print(\"Saving model....\")\n",
    "    path = os.path.join(OUTPUT_DIR, \"temp_dict.pkl\")\n",
    "    print(f\"saving to {path}\")\n",
    "    with open(path, 'wb') as p_file:\n",
    "        pickle.dump(final_model, p_file)\n",
    "            \n",
    "    print('Training Job is completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "69440412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "recommender_image_uri = '120582440665.dkr.ecr.ap-southeast-1.amazonaws.com/cutomer-churn-training-image:latest'\n",
    "    \n",
    "estimator = Estimator(image_uri=recommender_image_uri,\n",
    "                      role=role,\n",
    "                      sagemaker_session=sess,                                  # Technical object\n",
    "                      output_path=model_path,\n",
    "                      base_job_name='cutomer-churn-training-job',\n",
    "                      input_mode='File',                                       # Copy the dataset and then train    \n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=training_instance_type,\n",
    "                      debugger_hook_config=False,\n",
    "                      disable_profiler = True,\n",
    "                      metric_definitions=[\n",
    "                          # Only 40 Metrics can be accomodated\n",
    "                            {'Name': 'acc_train:' , 'Regex': 'acc_train:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'prec_train' , 'Regex': 'prec_train:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'rec_train:' , 'Regex': 'rec_train:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'f1_sc_train' , 'Regex': 'f1_sc_train:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'acc_test:' , 'Regex': 'acc_test:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'prec_test' , 'Regex': 'prec_test:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'rec_test:' , 'Regex': 'rec_test:([-+]?[0-9]*\\.?[0-9]+)'},\n",
    "                            {'Name': 'f1_sc_test' , 'Regex': 'f1_sc_test:([-+]?[0-9]*\\.?[0-9]+)'}\n",
    "                      ],\n",
    "                      #tags = generic_tags + [{'Key': 'JobType', 'Value': 'Training'}],\n",
    "                      #subnets = subnets.split(':'),\n",
    "                      #security_group_ids = security_group_ids.split(':')\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3fa7a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train = TrainingStep(\n",
    "        name=\"cutomer-churn-training-job\",\n",
    "        estimator=estimator,\n",
    "        inputs = {\n",
    "            \"training\": TrainingInput(\n",
    "                s3_data=step_preprocess.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "           \"test\": TrainingInput(\n",
    "                s3_data=step_preprocess.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            )\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1210a0a",
   "metadata": {},
   "source": [
    "## Define a Create Model Step to Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6920b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    image_uri=recommender_image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d7b427b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    accelerator_type=\"ml.eia1.medium\",\n",
    ")\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"Customer-churn-CreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e857edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"120582440665.dkr.ecr.ap-southeast-1.amazonaws.com/customer-churn-inference-image:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dcac605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_create_model = CreateModelStep(\n",
    "        name=\"Customer-churn-CreateModel\",\n",
    "        model=Model(image, \n",
    "                    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                    role = role,\n",
    "                    sagemaker_session = sagemaker_session,\n",
    "                    ),\n",
    "        inputs=CreateModelInput(instance_type=\"ml.m5.large\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c4f62d",
   "metadata": {},
   "source": [
    "## Define a Transform Step to Perform Batch Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a32bf1",
   "metadata": {},
   "source": [
    "without flask app (not the seperate inference job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6d842296",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = step_create_model.properties.ModelName\n",
    "\n",
    "transformer = Transformer(model_name=model_name,\n",
    "                          instance_count=1,\n",
    "                          strategy='SingleRecord',\n",
    "                          #max_payload=10, #Optional\n",
    "                          assemble_with=\"Line\",\n",
    "                          instance_type=\"ml.m5.12xlarge\",\n",
    "                          output_path=f\"s3://{default_bucket}/customer_churn/transform\",\n",
    "                          #tags = generic_tags + [{'Key': 'JobName', 'Value': 'Inference'}]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9511b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data = os.path.join(input_path,\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "df6cbb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_transform = TransformStep(\n",
    "        name=\"Customer-churn-BatchTransform\",\n",
    "        transformer=transformer,\n",
    "        inputs=TransformInput(data=\"s3://dlk-cloud-tier-10-preprocessed-ml-dev/PREPAID_CHURN/X/X.csv\",\n",
    "                              #data = step_preprocess.properties.ProcessingOutputConfig.Outputs[\"inference\"].S3Output.S3Uri,\n",
    "                              split_type=\"Line\",\n",
    "                              content_type=\"text/csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcc658",
   "metadata": {},
   "source": [
    "## Define a Register Model Step to Create a Model Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92469c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name = f\"CustomerChurnModelPackageGroupName\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6545b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_register = RegisterModel(\n",
    "    name=\"Customer-churn-RegisterModel\",\n",
    "    estimator=estimator,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    #model_metrics=model_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d913c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_name = f\"CustomerChurnPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        model_path,\n",
    "    ],\n",
    "    steps=[step_preprocess,\n",
    "           step_train, \n",
    "           #step_eval, \n",
    "           step_create_model,\n",
    "           step_register,\n",
    "           step_transform\n",
    "          ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "eb6dee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:pipeline/customerchurnpipeline',\n",
       " 'ResponseMetadata': {'RequestId': '820d98db-0819-480f-ad2d-d68287dde2d4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '820d98db-0819-480f-ad2d-d68287dde2d4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '94',\n",
       "   'date': 'Tue, 01 Feb 2022 15:26:48 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ba39f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "aae76719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:pipeline/customerchurnpipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:pipeline/customerchurnpipeline/execution/nhrihfdef870',\n",
       " 'PipelineExecutionDisplayName': 'execution-1643729211142',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'customerchurnpipeline',\n",
       "  'TrialName': 'nhrihfdef870'},\n",
       " 'CreationTime': datetime.datetime(2022, 2, 1, 15, 26, 51, 48000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 2, 1, 15, 26, 51, 48000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {},\n",
       " 'LastModifiedBy': {},\n",
       " 'ResponseMetadata': {'RequestId': '04c04902-1046-42a1-8316-a4b0b91cbc93',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '04c04902-1046-42a1-8316-a4b0b91cbc93',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '514',\n",
       "   'date': 'Tue, 01 Feb 2022 15:26:50 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1a959056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'Customer-churn-BatchTransform',\n",
       "  'StartTime': datetime.datetime(2022, 2, 1, 15, 42, 6, 260000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 2, 1, 15, 49, 46, 175000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'TransformJob': {'Arn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:transform-job/pipelines-nhrihfdef870-customer-churn-batch-e19fncink4'}}},\n",
       " {'StepName': 'Customer-churn-CreateModel',\n",
       "  'StartTime': datetime.datetime(2022, 2, 1, 15, 42, 4, 754000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 2, 1, 15, 42, 5, 640000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:model/pipelines-nhrihfdef870-customer-churn-creat-pvwcy8a6qn'}}},\n",
       " {'StepName': 'Customer-churn-RegisterModel',\n",
       "  'StartTime': datetime.datetime(2022, 2, 1, 15, 42, 4, 754000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 2, 1, 15, 42, 5, 844000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:model-package/customerchurnmodelpackagegroupname/4'}}},\n",
       " {'StepName': 'cutomer-churn-training-job',\n",
       "  'StartTime': datetime.datetime(2022, 2, 1, 15, 34, 18, 658000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 2, 1, 15, 42, 3, 849000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:training-job/pipelines-nhrihfdef870-cutomer-churn-traini-rie0pyndij'}}},\n",
       " {'StepName': 'customer-churn-preprocessing',\n",
       "  'StartTime': datetime.datetime(2022, 2, 1, 15, 26, 52, 497000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 2, 1, 15, 34, 18, 30000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:processing-job/pipelines-nhrihfdef870-customer-churn-prepr-hwfgaenswf'}}}]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9600f6",
   "metadata": {},
   "source": [
    "## Lineage\n",
    "Review the lineage of the artifacts generated by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "70a6f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'customer-churn-preprocessing', 'StartTime': datetime.datetime(2022, 2, 1, 15, 26, 52, 497000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2022, 2, 1, 15, 34, 18, 30000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'AttemptCount': 0, 'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:processing-job/pipelines-nhrihfdef870-customer-churn-prepr-hwfgaenswf'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...3d72e7129923/input/code/preprocessing.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...URN/WA_Fn-UseC_-Telco-Customer-Churn.csv</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12058...cutomer-churn-preprocessing-image:latest</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://...-10-preprocessed-ml-dev/PREPAID_CHURN/y/</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...-10-preprocessed-ml-dev/PREPAID_CHURN/X/</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...3d72e7129923/input/code/preprocessing.py     Input  DataSet   \n",
       "1  s3://...URN/WA_Fn-UseC_-Telco-Customer-Churn.csv     Input  DataSet   \n",
       "2  12058...cutomer-churn-preprocessing-image:latest     Input    Image   \n",
       "3  s3://...-10-preprocessed-ml-dev/PREPAID_CHURN/y/    Output  DataSet   \n",
       "4  s3://...-10-preprocessed-ml-dev/PREPAID_CHURN/X/    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3         Produced     artifact  \n",
       "4         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'cutomer-churn-training-job', 'StartTime': datetime.datetime(2022, 2, 1, 15, 34, 18, 658000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2022, 2, 1, 15, 42, 3, 849000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'AttemptCount': 0, 'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:training-job/pipelines-nhrihfdef870-cutomer-churn-traini-rie0pyndij'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...-10-preprocessed-ml-dev/PREPAID_CHURN/y/</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...-10-preprocessed-ml-dev/PREPAID_CHURN/X/</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12058....com/cutomer-churn-training-image:latest</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://...rn-traini-RIe0PYndij/output/model.tar.gz</td>\n",
       "      <td>Output</td>\n",
       "      <td>Model</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...-10-preprocessed-ml-dev/PREPAID_CHURN/y/     Input  DataSet   \n",
       "1  s3://...-10-preprocessed-ml-dev/PREPAID_CHURN/X/     Input  DataSet   \n",
       "2  12058....com/cutomer-churn-training-image:latest     Input    Image   \n",
       "3  s3://...rn-traini-RIe0PYndij/output/model.tar.gz    Output    Model   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'Customer-churn-RegisterModel', 'StartTime': datetime.datetime(2022, 2, 1, 15, 42, 4, 754000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2022, 2, 1, 15, 42, 5, 844000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'AttemptCount': 0, 'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:model-package/customerchurnmodelpackagegroupname/4'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...rn-traini-RIe0PYndij/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12058....com/cutomer-churn-training-image:latest</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>customerchurnmodelpackagegroupname-4-Approved-...</td>\n",
       "      <td>Input</td>\n",
       "      <td>Approval</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CustomerChurnModelPackageGroupName-1643722245-...</td>\n",
       "      <td>Output</td>\n",
       "      <td>ModelGroup</td>\n",
       "      <td>AssociatedWith</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name/Source Direction        Type  \\\n",
       "0   s3://...rn-traini-RIe0PYndij/output/model.tar.gz     Input       Model   \n",
       "1   12058....com/cutomer-churn-training-image:latest     Input       Image   \n",
       "2  customerchurnmodelpackagegroupname-4-Approved-...     Input    Approval   \n",
       "3  CustomerChurnModelPackageGroupName-1643722245-...    Output  ModelGroup   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo       action  \n",
       "3   AssociatedWith      context  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'Customer-churn-CreateModel', 'StartTime': datetime.datetime(2022, 2, 1, 15, 42, 4, 754000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2022, 2, 1, 15, 42, 5, 640000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'AttemptCount': 0, 'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:model/pipelines-nhrihfdef870-customer-churn-creat-pvwcy8a6qn'}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'Customer-churn-BatchTransform', 'StartTime': datetime.datetime(2022, 2, 1, 15, 42, 6, 260000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2022, 2, 1, 15, 49, 46, 175000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'AttemptCount': 0, 'Metadata': {'TransformJob': {'Arn': 'arn:aws:sagemaker:ap-southeast-1:120582440665:transform-job/pipelines-nhrihfdef870-customer-churn-batch-e19fncink4'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...rn-traini-RIe0PYndij/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12058...om/customer-churn-inference-image:latest</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...reprocessed-ml-dev/PREPAID_CHURN/X/X.csv</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://...-1-120582440665/customer_churn/transform</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...rn-traini-RIe0PYndij/output/model.tar.gz     Input    Model   \n",
       "1  12058...om/customer-churn-inference-image:latest     Input    Image   \n",
       "2  s3://...reprocessed-ml-dev/PREPAID_CHURN/X/X.csv     Input  DataSet   \n",
       "3  s3://...-1-120582440665/customer_churn/transform    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4aa879",
   "metadata": {},
   "source": [
    "## Clean-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa6e6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b3b1eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.delete_pipeline(\n",
    "    PipelineName='CustomerChurnPipeline',\n",
    "    ClientRequestToken='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da0a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
